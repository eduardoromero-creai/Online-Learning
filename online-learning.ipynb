{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2947792,"sourceType":"datasetVersion","datasetId":1807255}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Efecto del Aprendizaje en Línea en el Desempeño de Modelos con Distintos Perfiles de Sesgo y Varianza\n\n## Objetivo\n\nEvaluar como el aprendizaje en línea modifica el rendimiento y la estabilidad de modelos de distinto sesgo/variance, comparandolo con aprendizaje por lotes (batch), bajo escenarios estacionarios y no estacionarios (concept draft)\n\n## Importancia de On Line Learning\n\nEn la vida real, las propiedades estadísticas de la variable objetivo cambian con el tiempo. Lo que invalida el aprendizaje de cualquier modelo, llevando a caidas en la precisión de la predicción.\n\nEn terminos simples, esto quiere decir que el \"concepto\" o los patrones que el modelo de Machine Learning originalmente aprendió ya no reflejan la realidad actual de los datos. Esto pasa en distintos escenarios de la vida real donde el entorno o el comportamiento que fueron modelado evoluciona.\n\nEstos cambios de comportamiento suelen llamarse **drift** y hay distintos tipos:\n\n- **Sudden dift:** Cambios abruptos y significativos en el concepto.\n- **Gradual drift:** Cambios lentos y paulatinos en el concepto a través del tiempo.\n- **Recurrent drift:** Cambios que tienden a regresar a conceptos observados anteriormente.\n- **Incremental drift:** Una serie de cambios pequeños que se van acumulando con el tiempo.\n\nExisten distintas formas de solventar los cambios de concepto:\n\n- **Detección de drift:** Identificar las variables que estén causando drift en el concepto.\n- **Adaptación del modelo:** Re-entrenar los modelos cada vez que se detecta una caida en el desempeño de los modelos.\n- **Entendimiento de la causa raíz:** Identificar la causa raíz y crear catacterísticas/variables que sean capaces de capturar estos efectos para modelarlos.\n\nEl aprendizaje en línea adapta los modelos en cada nueva iteración de predicción, siguiendo la siguiente lógica:\n\n```\n1. Transforma el input\n2. Predice el valor del patrón de entrada\n3. Aprende sobre el patrón de entrada\n```\n\n**¿Por qué primero predice y luego aprende?** Esto se hace porque, en un escenario real, no conocemos aún el valor real de la entrada, por lo que no es posible pasar a una fase de aprendizaje. Una vez se confirma el valor real, entonces podemos pasar a la gase de aprendizaje del patrón predicho y obtener un score.\n\n>**NOTA IMPORTANTE**\n>\n>Ninguno de los algoritmos mostrados aquí tuvo ajuste fino, se tuvo la precaución de mantenerlos en su versión más vanilla para poder hacer una comparativa justa.\n>\n>Tampoco hubo un trabajo muy profundo en tratamiento de datos y, por cada algoritmo, se mantiene una versión compatible de los mismos tratamiento, haciendo que todos compartan características.\n>\n>No se comparten pipelines de tratamiento de datos ni entre algoritmos de scikit-learn, tampoco entre algoritmos de river y tampoco cruzado. Hay una limitante en cuanto a los algoritmos que son compatibles entre ambas librerías y, en muchas ocasiones, la compatibilidad trae problemas difíciles de corregir.\n\n## Recordatorio: Sesgo vs Varianza\n\n**Sesgo (Bias):** El error que introduce el modelo al simplificar demasiado la realidad (subajuste). Un modelo con alto sesgo no captura las relaciones complejas en los datos.\n\n**Varianza (Variance):** La sensibilidad del modelo a las pequeñas fluctuaciones en el conjunto de datos de entrenamiento (sobreajuste). Un modelo con alta varianza captura el \"ruido\" en lugar de la \"señal\".\n\nCada modelo de machine learning tiende más al sesgo o a la varianza, dependiendo de cada algoritmos.\n\nPor ejemplo, la regresión lineal es un modelo de alto sesgo y baja varianza, esto quiere decir que el modelo es muy estable y consistente (alto sesgo), pero es muy rígico (baja varianza). Lo que provoca que tenga problemas de generalización para problemas complejos.\n\nPor otro lado, un árbol de decisión tiene bajo sesgo y alta varianza, lo que se traduce en un modelo que tiene la capacidad de generalizar adecuadamente los problemas (alta varianza), pero no es estable, ni consistente (bajo sesgo).","metadata":{}},{"cell_type":"code","source":"! pip install river","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:39.459605Z","iopub.execute_input":"2025-09-18T17:01:39.460370Z","iopub.status.idle":"2025-09-18T17:01:42.962353Z","shell.execute_reply.started":"2025-09-18T17:01:39.460338Z","shell.execute_reply":"2025-09-18T17:01:42.961146Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importación de librerias\n\nHay dos librerías importantes aquí, `scikit-learn` y `river`.\n\nScikit-learn se utiliza para aprendizaje en lotes (aunque también tiene implementaciones de aprendizaje en línea)\n\nRiver es una librería especializada para aprendizaje en línea, con algoritmos adaptados a este tipo de aprendizaje.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn import linear_model\nfrom sklearn import ensemble\nfrom sklearn import pipeline\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn import compose\nfrom sklearn import feature_extraction\nfrom sklearn import tree\n\nfrom river import stream, optim\nfrom river import linear_model as river_lm\nfrom river import compose as river_compose\nfrom river import preprocessing as river_preprocessing\nfrom river import compat as river_compat\nfrom river import feature_extraction as rfe\nfrom river import tree as river_tree\nfrom river import ensemble as river_ensemble\nfrom river import metrics as river_metrics\nfrom river import evaluate as river_eval\n\nfrom category_encoders import WOEEncoder\nimport xgboost as xgb\n\n# Suprimir warning del output para no ensuciar la vista de resultados\nimport warnings\nwarnings.filterwarnings(action='ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:42.964584Z","iopub.execute_input":"2025-09-18T17:01:42.964864Z","iopub.status.idle":"2025-09-18T17:01:42.972671Z","shell.execute_reply.started":"2025-09-18T17:01:42.964834Z","shell.execute_reply":"2025-09-18T17:01:42.971710Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Lectura de fuente de datos","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\n    \"/kaggle/input/vehicle-claim-fraud-detection/fraud_oracle.csv\"\n)\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:42.973598Z","iopub.execute_input":"2025-09-18T17:01:42.973854Z","iopub.status.idle":"2025-09-18T17:01:43.109478Z","shell.execute_reply.started":"2025-09-18T17:01:42.973834Z","shell.execute_reply":"2025-09-18T17:01:43.108478Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preparación del dataset\n\nSe prepara la información se la siguiente manera\n\n1. Se separan las variables que son binarias y se convierten.\n2. Las columnas de fecha se convierten a números.\n3. Se remueven los registros que no coinciden con este formato (al momento de la exploración, solamente se detecta un registro con estas características)\n4. Se separa la variable objetivo de las variables predictoras.","metadata":{}},{"cell_type":"code","source":"encoder_sex = {\"Male\":0, \"Female\":1}\nencoder_accident_area = {\"Urban\":0, \"Rural\":1}\nencoder_fault = {\"Policy Holder\":0, \"Third Party\":1}\nencoder_agent_type = {\"External\":0, \"Internal\":1}\nencoder_police_report = {\"No\":0, \"Yes\":1}\nencoder_witness_present = {\"No\":0, \"Yes\":1}\n\nencoder_month = {\"Jan\":1,\"Feb\":2,\"Mar\":3,\"Apr\":4,\"May\":5,\"Jun\":6,\n                 \"Jul\":7,\"Aug\":8,\"Sep\":9,\"Oct\":10,\"Nov\":11,\"Dec\":12}\nencoder_week = {'Monday': 1,'Tuesday': 2,'Wednesday': 3, 'Thursday': 4,\n                'Friday': 5,'Saturday': 6,'Sunday': 7,}\n\ndf = df.query(\"DayOfWeekClaimed != '0'\")\n\ny = df[[\"FraudFound_P\"]]\nX = df.drop(columns=\"FraudFound_P\")\n\nX = X.assign(\n    Sex = lambda df: df.Sex.map(encoder_sex),\n    AccidentArea = lambda df: df.AccidentArea.map(encoder_accident_area),\n    Fault = lambda df: df.Fault.map(encoder_fault),\n    AgentType = lambda df: df.AgentType.map(encoder_agent_type),\n    PoliceReportFiled = lambda df: df.PoliceReportFiled.map(encoder_police_report),\n    WitnessPresent = lambda df: df.WitnessPresent.map(encoder_witness_present),\n    Month = lambda df: df.Month.map(encoder_month),\n    MonthClaimed = lambda df: df.MonthClaimed.map(encoder_month),\n    DayOfWeek = lambda df: df.DayOfWeek.map(encoder_week),\n    DayOfWeekClaimed = lambda df: df.DayOfWeekClaimed.map(encoder_week)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:43.110360Z","iopub.execute_input":"2025-09-18T17:01:43.110590Z","iopub.status.idle":"2025-09-18T17:01:43.153617Z","shell.execute_reply.started":"2025-09-18T17:01:43.110571Z","shell.execute_reply":"2025-09-18T17:01:43.152231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Variables binarias\nbinary_features = [\"Sex\",\"AccidentArea\",\"Fault\",\"AgentType\",\"PoliceReportFiled\",\"WitnessPresent\"]\n\n# Variables con hasta 5 valores únicos\n# Deductible es una variable numérica, pero tiene 5 valores únicos. Como no se esperan valores continuos, entonces\n# se trata como una variable categórica\ncategorical_features = [\"MaritalStatus\",\"VehicleCategory\",\"BasePolicy\",\"Year\",\"Days_Policy_Claim\",\n                        \"Days_Policy_Accident\",\"PastNumberOfClaims\",\"NumberOfSuppliments\",\n                        \"AddressChange_Claim\",\"NumberOfCars\",\"Deductible\"]\n\n# Variables numéricas\nnumerical_features = [\"Age\"]\n\n# Variables de alta dimensionalidad, estas son columnas con más de 5 valores únicos\nhigh_dimensional_features = [\"Make\",\"PolicyType\",\"VehiclePrice\",\"AgeOfVehicle\",\"AgeOfPolicyHolder\"]\n\n# Variables de fecha o cíclicas\ndate_features = [\"Month\",\"DayOfWeek\",\"DayOfWeekClaimed\",\"MonthClaimed\",\"WeekOfMonthClaimed\",\"WeekOfMonth\"]\n\n# Variables de las que se desconoce su naturaleza, estas NO son consideradas en el modelado\nunkown_features = [\"RepNumber\",\"PolicyNumber\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:43.156733Z","iopub.execute_input":"2025-09-18T17:01:43.157000Z","iopub.status.idle":"2025-09-18T17:01:43.165236Z","shell.execute_reply.started":"2025-09-18T17:01:43.156979Z","shell.execute_reply":"2025-09-18T17:01:43.163877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cyclic_date_encoding(X):\n    \"\"\"Función para codificar las variables cíclicas (o de fechas)\n    Con esta función se captura la naturaleza cíclica de las fechas\n    utilizando las funciones de seno y coseno\n    \"\"\"\n    results = []\n    for col in X.columns:\n        if col in [\"Month\",\"MonthClaimed\"]:\n            max_val = 12\n        elif col in [\"DayOfWeek\",\"DayOfWeekClaimed\"]:\n            max_val = 7\n        elif col in [\"WeekOfMonthClaimed\",\"WeekOfMonth\"]:\n            max_val = 5\n\n        sin_val = np.sin(2 * np.pi * X[col] / max_val)\n        cos_val = np.cos(2 * np.pi * X[col] / max_val)\n        results.extend([sin_val, cos_val])\n\n    return np.column_stack(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:43.166424Z","iopub.execute_input":"2025-09-18T17:01:43.166916Z","iopub.status.idle":"2025-09-18T17:01:43.194964Z","shell.execute_reply.started":"2025-09-18T17:01:43.166875Z","shell.execute_reply":"2025-09-18T17:01:43.194150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def hasher_transform(X: pd.DataFrame):\n    fh = feature_extraction.FeatureHasher(n_features=5, input_type=\"string\")\n    # convertir cada valor a string porque FH espera texto\n    X_str = X.astype(str).to_dict(orient=\"records\")\n    return fh.transform(X_str).toarray()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:43.195986Z","iopub.execute_input":"2025-09-18T17:01:43.196288Z","iopub.status.idle":"2025-09-18T17:01:43.216189Z","shell.execute_reply.started":"2025-09-18T17:01:43.196263Z","shell.execute_reply":"2025-09-18T17:01:43.215073Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Entrenamiento\n\n## Modelos de Alto Sesgo y Baja Varianza\n\n### Aprendizaje por Lotes\n\nPara este primer ejercicio, se toma el modelo de regresión logística sin características polinómicas.\n\n> ¿Qué son las características polinómicas y por qué no se toman en cuenta?\n>\n> Las variables polinómicas ayudan a los modelos lineales a aprender fronteras de decisión no lineales. Esto es, capturar relaciones complejas (o no lineales), lo que mejora la precisión del modelo.\n>\n> Al ampliar su frontera de decisión aumentamos su varianza mientras reducimos sus sesgo, al agregar técnicas de regularización y escalado de variables, equilibramos el modelo aumentando ligeramente el sesgo y reduciendo la varianza. Evitando que este tenga problemas de sobre ajuste.","metadata":{}},{"cell_type":"code","source":"# Pipeline de transformación por columna\nct = compose.ColumnTransformer(\n    [(\"binary_features\", \"passthrough\", binary_features),\n     (\"categorical_features\", preprocessing.OneHotEncoder(), categorical_features),\n     (\"numerical_features\", preprocessing.StandardScaler(), numerical_features),\n     (\"high_dimensional_features\", preprocessing.FunctionTransformer(hasher_transform, validate=False), high_dimensional_features),\n     (\"date_features\", preprocessing.FunctionTransformer(cyclic_date_encoding), date_features)],\n    remainder=\"drop\"\n)\n\n# Pipeline de entrenamiento por lotes (batch)\nlr_model = pipeline.Pipeline([\n    (\"transformer\", ct),\n    (\"model\", linear_model.LogisticRegression(solver=\"lbfgs\"))\n])\n\ncv = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\nscorer = metrics.make_scorer(metrics.roc_auc_score)\nscores = model_selection.cross_val_score(lr_model, X, y, scoring=scorer, cv=cv)\n\nLR_ROC_AUC_BATCH = scores.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:43.217081Z","iopub.execute_input":"2025-09-18T17:01:43.217318Z","iopub.status.idle":"2025-09-18T17:01:46.901749Z","shell.execute_reply.started":"2025-09-18T17:01:43.217300Z","shell.execute_reply":"2025-09-18T17:01:46.898277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Aprendizaje en línea\n\nEl pipeline de tratamiento de datos hace la mismas transformaciones que el pipeline de aprendizaje por lotes.","metadata":{}},{"cell_type":"code","source":"def cyclic_date_encoding_river(X):\n    \"\"\"Función para codificar las variables cíclicas (o de fechas)\n    Con esta función se captura la naturaleza cíclica de las fechas\n    utilizando las funciones de seno y coseno\n    \"\"\"\n    results = {}\n    \n    for col in X.keys():\n        if col in [\"Month\",\"MonthClaimed\"]:\n            max_val = 12\n        elif col in [\"DayOfWeek\",\"DayOfWeekClaimed\"]:\n            max_val = 7\n        elif col in [\"WeekOfMonthClaimed\",\"WeekOfMonth\"]:\n            max_val = 5\n        else:\n            continue\n\n        sin_val = np.sin(2 * np.pi * X[col] / max_val)\n        cos_val = np.cos(2 * np.pi * X[col] / max_val)\n        results[col+f\"_{X[col]}_sin\"] = sin_val\n        results[col+f\"_{X[col]}_cos\"] = cos_val\n\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:46.902762Z","iopub.execute_input":"2025-09-18T17:01:46.903345Z","iopub.status.idle":"2025-09-18T17:01:46.913484Z","shell.execute_reply.started":"2025-09-18T17:01:46.903314Z","shell.execute_reply":"2025-09-18T17:01:46.911661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_pipe = river_compose.Select(\"Age\") | river_preprocessing.StandardScaler()\ncategorical_pipe = river_compose.Select(*(categorical_features+high_dimensional_features)) | river_preprocessing.OneHotEncoder()\nhigh_dim_pipe = river_compose.Select(*high_dimensional_features) | river_preprocessing.FeatureHasher(n_features=5, seed=42)\ndate_pipe = river_compose.Select(*date_features) | river_compose.FuncTransformer(cyclic_date_encoding_river)\n\ntransformer = numerical_pipe + categorical_pipe + date_pipe + high_dim_pipe\n\nlog_reg = river_lm.LogisticRegression()\n\ny_true = []\ny_pred = []\n\nfor xi, yi in stream.iter_pandas(X, y.FraudFound_P):\n    transformer.learn_one(xi)\n    xi_transformed = transformer.transform_one(xi)\n\n    yi_pred = log_reg.predict_proba_one(xi_transformed)\n    log_reg.learn_one(xi_transformed, yi)\n\n    y_true.append(yi)\n    y_pred.append(yi_pred[True])\n\n# print(f\"ROC AUC: {metrics.roc_auc_score(y_true, y_pred):.3f}\")\nLR_ROC_AUC_ONLINE = metrics.roc_auc_score(y_true, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:46.914284Z","iopub.execute_input":"2025-09-18T17:01:46.914591Z","iopub.status.idle":"2025-09-18T17:01:52.712315Z","shell.execute_reply.started":"2025-09-18T17:01:46.914565Z","shell.execute_reply":"2025-09-18T17:01:52.711418Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modelo de Bajo Sesgo y Alta Varianza\n\n### Aprendizaje por lotes","metadata":{}},{"cell_type":"code","source":"# Pipeline de transformación por columna\nct = compose.ColumnTransformer(\n    [(\"binary_features\", \"passthrough\", binary_features),\n     (\"categorical_features\", preprocessing.OneHotEncoder(), categorical_features),\n     (\"numerical_features\", preprocessing.StandardScaler(), numerical_features),\n     (\"high_dimensional_features\", preprocessing.FunctionTransformer(hasher_transform, validate=False), high_dimensional_features),\n     (\"date_features\", preprocessing.FunctionTransformer(cyclic_date_encoding), date_features)],\n    remainder=\"drop\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:52.713513Z","iopub.execute_input":"2025-09-18T17:01:52.713792Z","iopub.status.idle":"2025-09-18T17:01:52.719608Z","shell.execute_reply.started":"2025-09-18T17:01:52.713771Z","shell.execute_reply":"2025-09-18T17:01:52.718732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pipeline de entrenamiento por lotes (batch)\ntree_model = pipeline.Pipeline([\n    (\"transformer\", ct),\n    (\"model\", tree.DecisionTreeClassifier(max_depth=None, random_state=42))\n])\n\ncv = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\nscorer = metrics.make_scorer(metrics.roc_auc_score)\nscores = model_selection.cross_val_score(tree_model, X, y, scoring=scorer, cv=cv)\n\nTREE_ROC_AUC_BATCH = scores.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:52.721190Z","iopub.execute_input":"2025-09-18T17:01:52.721461Z","iopub.status.idle":"2025-09-18T17:01:53.825008Z","shell.execute_reply.started":"2025-09-18T17:01:52.721443Z","shell.execute_reply":"2025-09-18T17:01:53.824374Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Aprendizaje en línea","metadata":{}},{"cell_type":"code","source":"# numéricas: NO necesitas escalarlas\nnumerical_pipe = river_compose.Select(\"Age\")\n\n# categóricas: puedes pasar strings directo al árbol\ncategorical_pipe = river_compose.Select(*categorical_features)\n\n# features de alta dimensionalidad:\nhigh_dim_pipe = river_compose.Select(*high_dimensional_features)\n\n# fechas: puedes mantener tu encoding cíclico\ndate_pipe = river_compose.Select(*date_features) | river_compose.FuncTransformer(cyclic_date_encoding_river)\n\n# combinas todo\ntransformer = numerical_pipe + categorical_pipe + date_pipe + high_dim_pipe\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:53.826090Z","iopub.execute_input":"2025-09-18T17:01:53.826443Z","iopub.status.idle":"2025-09-18T17:01:53.832144Z","shell.execute_reply.started":"2025-09-18T17:01:53.826415Z","shell.execute_reply":"2025-09-18T17:01:53.831125Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_true = []\ny_pred = []\n\ntree_clf = river_tree.HoeffdingTreeClassifier(max_depth=None)\n\nfor xi, yi in stream.iter_pandas(X, y.FraudFound_P):\n    # Transformación de features\n    transformer.learn_one(xi)\n    xi_transformed = transformer.transform_one(xi)\n\n    # Predicción antes de aprender (prequential)\n    yi_proba = tree_clf.predict_proba_one(xi_transformed)\n\n    # Si no hay predicción todavía -> asignar prob. por defecto\n    if yi_proba:\n        y_pred.append(yi_proba.get(True, 0.0))  # prob. de clase positiva\n    else:\n        y_pred.append(0.5)  # neutro\n\n    y_true.append(yi)\n\n    # Ahora sí, actualizar el modelo\n    tree_clf.learn_one(xi_transformed, yi)\n\nTREE_ROC_AUC_ONLINE = metrics.roc_auc_score(y_true, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:53.835077Z","iopub.execute_input":"2025-09-18T17:01:53.835365Z","iopub.status.idle":"2025-09-18T17:01:58.094848Z","shell.execute_reply.started":"2025-09-18T17:01:53.835342Z","shell.execute_reply":"2025-09-18T17:01:58.094114Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modelo Equilibrado\n\n### Aprendizaje por lotes","metadata":{}},{"cell_type":"code","source":"# Pipeline de transformación por columna\nct_xgb = compose.ColumnTransformer(\n    [(\"binary_features\", \"passthrough\", binary_features),\n     (\"categorical_features\", preprocessing.OneHotEncoder(handle_unknown=\"infrequent_if_exist\"), categorical_features),\n     (\"numerical_features\", \"passthrough\", numerical_features),\n     (\"high_dimensional_features\", preprocessing.OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), high_dimensional_features),\n     (\"date_features\", preprocessing.FunctionTransformer(cyclic_date_encoding), date_features)],\n    remainder=\"drop\"\n)\n\n# Pipeline de entrenamiento por lotes (batch)\ntree_model = pipeline.Pipeline([\n    (\"transformer\", ct_xgb),\n    (\"model\", xgb.XGBClassifier(tree_method=\"hist\", random_state=42, n_estimators=10))\n])\n\ncv = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\nscorer = metrics.make_scorer(metrics.roc_auc_score)\nscores = model_selection.cross_val_score(tree_model, X, y, scoring=scorer, cv=cv, error_score=\"raise\")\n\nXGB_ROC_AUC_BATCH = scores.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:58.095712Z","iopub.execute_input":"2025-09-18T17:01:58.095991Z","iopub.status.idle":"2025-09-18T17:01:58.864255Z","shell.execute_reply.started":"2025-09-18T17:01:58.095965Z","shell.execute_reply":"2025-09-18T17:01:58.863380Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Aprendizaje en línea","metadata":{}},{"cell_type":"code","source":"# numéricas: NO necesitas escalarlas\nnumerical_pipe = river_compose.Select(\"Age\")\n\n# categóricas: puedes pasar strings directo al árbol\ncategorical_pipe = river_compose.Select(*categorical_features)\n\n# features de alta dimensionalidad:\nhigh_dim_pipe = river_compose.Select(*high_dimensional_features)\n\n# fechas: puedes mantener tu encoding cíclico\ndate_pipe = river_compose.Select(*date_features) | river_compose.FuncTransformer(cyclic_date_encoding_river)\n\n# combinas todo\ntransformer = numerical_pipe + categorical_pipe + date_pipe + high_dim_pipe\n\nensemble_model = river_ensemble.ADWINBaggingClassifier(\n    model=(\n        transformer |\n        river_tree.HoeffdingTreeClassifier(max_depth=None)\n    ),\n    n_models=10,\n    seed=42\n)\n\nENSEMBLE_ROC_AUC_ONLINE = river_eval.progressive_val_score(stream.iter_pandas(X, y.FraudFound_P), ensemble_model, river_metrics.ROCAUC())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:01:58.865509Z","iopub.execute_input":"2025-09-18T17:01:58.865774Z","iopub.status.idle":"2025-09-18T17:03:07.701074Z","shell.execute_reply.started":"2025-09-18T17:01:58.865754Z","shell.execute_reply":"2025-09-18T17:03:07.699902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Para regresión logística:\")\nprint(f\"En su versión batch, ROC AUC {LR_ROC_AUC_BATCH:.2%}\")\nprint(f\"En su versión online, ROC AUC {LR_ROC_AUC_ONLINE:.2%}\")\nprint(\"\\n\")\nprint(\"Para árboles de decisión:\")\nprint(f\"En su versión batch, ROC AUC {TREE_ROC_AUC_BATCH:.2%}\")\nprint(f\"En su versión online, ROC AUC {TREE_ROC_AUC_ONLINE:.2%}\")\nprint(\"\\n\")\nprint(\"Para algoritmos ensamblados:\")\nprint(f\"En su versión batch (XGBoost), ROC AUC {XGB_ROC_AUC_BATCH:.2%}\")\nprint(f\"En su versión online (ADWIN Bagging), {ENSEMBLE_ROC_AUC_ONLINE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:03:07.701999Z","iopub.execute_input":"2025-09-18T17:03:07.702333Z","iopub.status.idle":"2025-09-18T17:03:07.709511Z","shell.execute_reply.started":"2025-09-18T17:03:07.702304Z","shell.execute_reply":"2025-09-18T17:03:07.708482Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Resultados y conclusiones\n\n**¿Por qué los modelos mejoran?** Esto ocurre por el concepto de drift y varianza. Por un lado, los modelos son capaces se aprender el drift del concepto que ocurre a través del tiempo, dando espacio en el streaming para aprender como se comporta con el tiempo.\n\nEl aprendizaje en línea reduce naturalmente la varianza debido a que estos aprenden por gradiente. Una sola estimación tiene mucho ruido y es imprecisa, llevando a una alta varianza, y esto mismo es lo que funge como regularización, agregando ruido a la actualización de los parámetros evitamos que el modelo sea rígido, similar a la regularización L2 o Dropout.\n\nEsto también mejora el sesgo, como el algoritmo no converge de forma \"perfecta\" y estable al óptimo global, en su lugar, oscila alrededor el óptimo global lo que mejora la generalización del problema.","metadata":{}}]}